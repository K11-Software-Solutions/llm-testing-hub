# Session 5: DeepEval for LLM Testing

This session introduces DeepEval, a framework for automated LLM evaluation with advanced metrics and assertions. You will:

- Use DeepEval to test LLM outputs for correctness, robustness, and safety
- Compare DeepEval with Promptfoo and LangTest
- Analyze results in HTML and text reports

## Structure
- `deepeval-framework-sample/` — DeepEval usage examples
- `deepeval-assertions-and-metrics/` — Assertion and metric examples
- `session5_deepeval_testresults/` — Output folder for DeepEval results

## Getting Started
1. Review the DeepEval sample and assertion folders.
2. Install dependencies:
   ```sh
   pip install deepeval openai
   ```
3. Set your API key as an environment variable:
   ```sh
   export OPENAI_API_KEY=your-key-here
   ```
4. Run DeepEval tests or use provided scripts for batch evaluation.

---
This session builds skills in advanced LLM evaluation using DeepEval.
